{"config":{"lang":["ja"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#overview","title":"Overview","text":"<p>Hello, my name is Tian Guo I am a computer science professor at WPI.  This site tracks academic research on microservice, as well as my personal reading notes. </p>"},{"location":"#why-this-site","title":"Why This Site?","text":"<p>A large part of my job is research, which roughly divides into keeping up with SToA, advising students, hands-on projects, and writing grant proposals. </p> <p>As an early 2023 new year resolution, I want to know more about microservice. I love reading and writing, so what would be a better way to keep me motivated.</p>"},{"location":"#relevant-conferences","title":"Relevant Conferences","text":"<p>Here are the list of conferences, not in any particular order, which I find each paper. </p> <ul> <li>EuroSys</li> <li>SoCC </li> <li>ICDCS </li> </ul>"},{"location":"benchmarks/Gan2019-ed/","title":"An Open-Source Benchmark Suite for Microservices and Their Hardware-Software Implications for Cloud &amp; Edge Systems","text":""},{"location":"benchmarks/Gan2019-ed/#overview","title":"Overview","text":"<p>Gan, Y. et al. 2019. An Open-Source Benchmark Suite for Microservices and Their Hardware-Software Implications for Cloud &amp; Edge Systems. Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (2019), 3\u201318.</p>"},{"location":"benchmarks/Gan2019-ed/#how-many-passes","title":"How Many Passes?","text":""},{"location":"benchmarks/Gan2019-ed/#my-ratings","title":"My Ratings","text":"<ul> <li>novelty: </li> <li>readability:  </li> <li>reproducability: </li> <li>practicability:  </li> </ul>"},{"location":"benchmarks/Gan2019-ed/#high-level-ideas","title":"High-Level Ideas","text":"<p>The paper introduces DeathStarBench, a benchmark suite for understanding the implications of microservices on the cloud system stack. </p> <p>Besides of the main contributions in the benchmark design and findings, the paper also includes a good background on microservices and its benefits. For example, one of the key benefits of microservices is provisioning agility---components of the same end-to-end application can be managed and scaled individually (not necessarily meant independently though). Microservices also allow resource managers better flexibility in packing services based on their resource bottlenecks. </p> <p>A lightweight distributed tracing system at the RPC granularity </p> <p>What does \"real users of the services\" mean? Does the authors recruit participants? </p> <p>It is intuitive that microserivces will be more network-bound than single-service applications. The key question is probably how to mitigate the network overhead. The paper proposes the idea of offloading RPC processing to an FPGA co-located in the same server can reduce the network overhead. </p> <p>The paper shows the implication on microservice performance when a component is mismanaged and demonstrates that existing resource management techniques (determining when to scale and how much to scale) do not work well for microservices. Though it does not look like the paper has suggestions on how to improve on this front. </p>"},{"location":"benchmarks/Gan2019-ed/#key-novelties","title":"Key Novelties","text":"<p>Does DeathStarBench support dynamic call graph structures? </p>"},{"location":"benchmarks/Gan2019-ed/#evaluation-highlights","title":"Evaluation Highlights","text":""},{"location":"benchmarks/Gan2019-ed/#a-closing-thought","title":"A Closing Thought","text":""},{"location":"load-balancing/Abdi2023-ik/","title":"Palette Load Balancing: Locality Hints for Serverless Functions","text":""},{"location":"load-balancing/Abdi2023-ik/#overview","title":"Overview","text":"<p>Abdi, M. et al. 2023. Palette Load Balancing: Locality Hints for Serverless Functions. Proceedings of the 18th European Conference on Computer Systems, (2023).</p>"},{"location":"load-balancing/Abdi2023-ik/#how-many-passes","title":"How Many Passes?","text":""},{"location":"load-balancing/Abdi2023-ik/#my-ratings","title":"My Ratings","text":"<ul> <li>novelty: </li> <li>readability:  </li> <li>reproducability: </li> <li>practicability:  </li> </ul>"},{"location":"load-balancing/Abdi2023-ik/#why-this-paper","title":"Why This Paper?","text":"<p>Microservices deployment often rely on serverless platforms. The efficiency of microservices therefore can be improved by having a better managed serverless platform. This paper showcases the Palette's benefit with a commonly-used microservice application (modified to use in-memory cache). In short, this paper does not specifically target at microservices, but it is well within the microservice ecosystem.</p> <p>A number of the authors are from the Microsoft Azure Systems Research and have done some great work in characterizing real-world serverless performance1 and improving serverless function caching2 from the cloud providers' perspective.</p>"},{"location":"load-balancing/Abdi2023-ik/#high-level-ideas","title":"High-Level Ideas","text":"<p>This paper tries to bring the data and computation closer for serverless applications. This improvement can be huge because serverless paradigm is often considered to be stateless. Any attempts to utilize existing serverless platforms to run data-intensive and stateful applications require some painstaking maneuver. Interesting readers can look at LambdaML 3 and FuncPipe 4, two recent works in supporting distributed training on serverless platforms from the cloud user's perspective. </p>"},{"location":"load-balancing/Abdi2023-ik/#key-novelties","title":"Key Novelties","text":""},{"location":"load-balancing/Abdi2023-ik/#evaluation-highlights","title":"Evaluation Highlights","text":""},{"location":"load-balancing/Abdi2023-ik/#a-closing-thought","title":"A Closing Thought","text":"<ol> <li> <p>Shahrad, M. et al. 2020. Serverless in the wild: Characterizing and optimizing the serverless workload at a large cloud provider. 2020 USENIX Annual Technical Conference (USENIX ATC 20) (2020), 205\u2013218.\u00a0\u21a9</p> </li> <li> <p>Romero, F. et al. 2021. Faa$T: A Transparent Auto-Scaling Cache for Serverless Applications. Proceedings of the ACM Symposium on Cloud Computing (Nov. 2021), 122\u2013137.\u00a0\u21a9</p> </li> <li> <p>Jiang, J. et al. 2021. Towards Demystifying Serverless Machine Learning Training. Proceedings of the 2021 International Conference on Management of Data (New York, NY, USA, Jun. 2021), 857\u2013871.\u00a0\u21a9</p> </li> <li> <p>Liu, Y. et al. 2022. FuncPipe: A Pipelined Serverless Framework for Fast and Cost-efficient Training of Deep Learning Models. arXiv [cs.DC]. To appear in ACM SIGMETRICS 2023.\u00a0\u21a9</p> </li> </ol>"},{"location":"resource-provisioning/Chow2022-pv/","title":"DeepRest: Deep Resource Estimation for Interactive Microservices","text":""},{"location":"resource-provisioning/Chow2022-pv/#overview","title":"Overview","text":"<p>Chow, K.-H. et al. 2022. DeepRest: deep resource estimation for interactive microservices. Proceedings of the Seventeenth European Conference on Computer Systems (New York, NY, USA, Mar. 2022), 181\u2013198.</p>"},{"location":"resource-provisioning/Chow2022-pv/#how-many-passes","title":"How Many Passes?","text":""},{"location":"resource-provisioning/Chow2022-pv/#my-ratings","title":"My Ratings","text":"<ul> <li>novelty:  </li> <li>readability:  </li> <li>reproducability:  </li> <li>practicability: unclear </li> </ul>"},{"location":"resource-provisioning/Chow2022-pv/#high-level-ideas","title":"High-Level Ideas","text":"<p>DeepRest models the resource utilization for each API endpoint. </p> <p>For the DeepRest feature engineering component, it takes distributed traces as input (see Figure 3 for an example), and outputs an execution topology graph (see Figure 5 for an example). Here, FrontendNGNIX is considered a component, and readTimeline is considered a operation name. </p> <p> </p> <p>The paper claims that its feature engineering method is more privacy-conserving compared to other NLP-based work that mines the information from the log, such as this work 1. One of the reasons listed is that DeepRest only requires component and operation names, and also hashes these strings.</p> <p>The authors made the observations that the resource utilization can be inferred from the innovation path, which can be identified via the DeepRest's feature engineering step. </p> <p>Once DeepRest obtains the feature vectors, it estimates the resource utilization time-series for all component in a specific resource dimension. That is, DeepRest models different resource utilization (e.g., disk or CPU) using a dedicate DNN model. These DNN models are referred to as DNN experts in this paper. </p> <p>The multi-expert resource estimator has the following three key design elements: </p> <ul> <li>Because different API endpoints can lead to different invocation paths, DeepRest uses a learnable weight vector to mask the input feature and then automatically associate attributes API invocation to corresopnding resource utilization. </li> <li>To account for the residual requests' impact on the current resource utilization, DeepRest uses a recurrent structure called Gated Recurrent Units (GRUs). The use of recurrent architecture also allows DeepRest to process variable input lengths; e.g., how much workload history to encode. </li> <li>To correlate the resource utlization of different components, DeepRest uses the attention mechanism to allow different DNN experts, which are in charge of one resource prediction, to communicate with each other. </li> </ul> <p>The following figure shows an example of one such DNN expert, for estimating the CPU resource for the Component 1. Assuming N components and each component has three resource dimensions (CPU, memory, and disk), DeepRest will end up with 3N experts. </p> <ul> <li>I am curious about the overhead, both in terms of training those experts and using these experts. How does the overhead compare to the resource provisioning savings from using DeepRest?</li> </ul> <p></p>"},{"location":"resource-provisioning/Chow2022-pv/#key-novelties","title":"Key Novelties","text":"<p>The main novelty is probably the API-based resource estimation model which nicely uses DL model design to solve system problem.  Note that DeepRest alone can't deliver guranteed end-user performance such as end-to-end latency. DeepRest has to be used in conjunction with auto-scaling policies, such as 2, to adjust resource allocation. </p> <p>The paper presnets two use cases, resource allocation and application sanity check, that can benefit from more accurate resource utilization provided by DeepRest. It is an interesting addons but I wish the paper has provided some more details on how to adjust reosurces and the impact on user-facing metrics like latency. The current analyses and comparions are still about the resource estimation accuracy, e.g., Figure 14. </p> <p></p> <p>Without showing actual provisioning costs and performance metrics, I am not sure how to interpret the presented results---for example, Figure 14 only shows that DeepRest has better predictions of how many resources are needed. However, questions such as whether the predicted resource gaps between different approaches are substaintial to save container resources or to impact end-user performances remain unclear. </p>"},{"location":"resource-provisioning/Chow2022-pv/#evaluation-highlights","title":"Evaluation Highlights","text":"<p>Like many microservice papers, this paper uses the microservice applications from the DeathStarBench.  To emulate the real-world workload, the authors used the graph data for Facebook for the Social Netowrk application and generated synthetic data for the Hotel Reservation application. Further, the authors used Locust, a load testing tool, to issue requests to the microservices. </p> <p>It is unclear what the container cluster used for microservice deployment looks like. The paper mentions: </p> <p>We deploy all microservices in separate Docker containers orchestrated by Kubernetes</p> <p>We collect seven days of data for application learning and train DeepRest ... </p> <p>Since the resource utilization data used to train DeepRest is container-dependent, I am curious the impact of different deployments, e.g., homogenous or heterogenous container resources.</p>"},{"location":"resource-provisioning/Chow2022-pv/#some-closing-thoughts","title":"Some Closing Thoughts","text":"<p>I quite like the API-driven resource estimation approach for microservice, and I think the paper has many clever points. </p> <p>What happens if for the same API endpoint, we start to observe different execution graphs depend on factors such as time and location? Will DeepRest still work well?</p> <p>DeepRest takes an interesting stance by decomposing the resource provisioning for microservices to each API end point. A lingering question is how well API-based resource provisioning compared to call graph-based (aggregated API endpoint executions) resource provisioning.</p> <p>It would be interesting to see how DeepRest compares to the power of prediction paper 2 in resource estimation. (Note: DeepRest was accepted to EuroSys 2022 which took place on April 5-8 2022, while SoCC 2022 submission deadline was June 17, 2022.) </p> <ol> <li> <p>Zhou, G. and Maas, M. 2021. Learning on distributed traces for data center storage systems. Proceedings of Machine Learning and Systems. 3, (Mar. 2021), 350\u2013364.\u00a0\u21a9</p> </li> <li> <p>Luo, S. et al. 2022. The power of prediction: microservice auto scaling via workload learning. Proceedings of the 13th Symposium on Cloud Computing (New York, NY, USA, Nov. 2022), 355\u2013369.\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"resource-provisioning/Luo2022-kp/","title":"The power of prediction: microservice auto scaling via workload learning","text":""},{"location":"resource-provisioning/Luo2022-kp/#overview","title":"Overview","text":"<p>Luo, S. et al. 2022. The power of prediction: microservice auto scaling via workload learning. Proceedings of the 13th Symposium on Cloud Computing (New York, NY, USA, Nov. 2022), 355\u2013369.</p>"},{"location":"resource-provisioning/Luo2022-kp/#how-many-passes","title":"How Many Passes?","text":""},{"location":"resource-provisioning/Luo2022-kp/#my-ratings","title":"My Ratings","text":"<ul> <li>novelty: </li> <li>readability:  </li> <li>reproducability: </li> <li>practicability:  </li> </ul>"},{"location":"resource-provisioning/Luo2022-kp/#high-level-ideas","title":"High-Level Ideas","text":"<p>This paper targets at the container-level resource management for microservice. One of the key arguements the paper makes is that:</p> <p>microservice workload per container is strongly correlated with the container\u2019s OS-level metrics (such as CPU and memory utilization), more than with application-level metrics (i.e., microservice response time). </p> <p>The paper uses this argument to justify the design of \"container-based auto-scaler\", which adjusts the resource configuration of each container in isolation. My current reactions are two-fold: </p> <ul> <li>If we scale microservice resources without considering call graph depdency, why do we need another auto-scaler for the microservice? How well do exisiting auto-scalers (that are not specifically designed for microservice) work? These questions are partially answered in the comparison to Google's Autopilot.</li> </ul> <p>Autopilot is a reactive scaler that uses a moving window to collect resource usage statistics in a most recent period, and scales resources based on these statistics, such as the average CPU and memory utilization.</p> <p></p> <p>The above figure shows that Madu (the auto-scaler proposed by this paper) uses only about 50% fewer CPU cores, while reducing the average latency by 90%, compared to Autopilot. </p> <p>I am not surprised that proactive auto-scalers (Madu) outperform reactive auto-scalers (Autopilot). Though I am curious about the following few aspects:</p> <ul> <li> <p>The paper only shows the comparison in terms of \"Normalized CPU Cores\", what about other resource dimensions? Is this because the \"two representative workload traces\" are CPU-bound? </p> <p>Since Alibaba clusters have deployed thousands of online services, far more than Death- StarBench, we selected only two representative workload traces.</p> </li> <li> <p>How much of the latency performance difference is due to the provisioning delay of reactive autoscaler, and how much is due to the resource estimation algorithms? (See Eq (11) for Madu's formulation.)</p> </li> <li>How does Madu compare to other proactive auto-scalers? This paper seems to equate \"proactive auto-scalers\" to workload prediction algorithm plus the online container scaling scheme (Section 4.3). Based on Figure 11, Madu's call per minute prediction accuracy is very close to that of Seq2Seq. I am a bit surprised to see the reported improvement.  <p>As illustrated in Fig. 15(c), Madu manages to reduce end-to-end latency by 35% on overage and 5.9\u00d7 at peak demands, compared to the other four scalers.</p> </li> </ul> <p></p> <p>The very slight workload prediction accuracy actually made a huge difference, as reported, under the heavily-flucated workload. So likely, the benefit is magnified by the many resource estimation \"triggers\".</p> <p></p> <ul> <li>Last but not the least, how should we understand the resource consumption and performance trade-off? In some cases, Madu launched more containers and had lower latency compared to Seq2Seq. That results are expected, and I am not sure they demonstrate Madu has better performance.   <p>Madu uses 3% more resources than the other four scalers due to underestimating peak demands. </p> </li> </ul>"},{"location":"resource-provisioning/Luo2022-kp/#key-novelties","title":"Key Novelties","text":"<p>The first one is probably the microservice workload predictor, built on top of the basic Seq2Seq model. (Sec 4.1) In particular, the paper describes \"a stochastic attention mechanism\" to model the workload uncertainty. The resulting model architecture is described in Figure 7 (not open sourced).</p>"},{"location":"resource-provisioning/Luo2022-kp/#evaluation-highlights","title":"Evaluation Highlights","text":"<ul> <li>The evaluation of the the workload predictor is based on the 1K microservices from the Alibaba's production clusters, which is open sourced and available here.</li> </ul>"},{"location":"resource-provisioning/Luo2022-kp/#a-closing-thought","title":"A Closing Thought","text":"<p>I still feel using container-based provisioning for microservice misses the key opportunity presented by dynamic call graph, and can easily lead to overprovisioning (maybe underprovisioning). </p>"}]}